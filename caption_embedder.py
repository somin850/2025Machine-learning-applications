# ğŸ“„ caption_embedder.py
"""
EmbeddingGemma ê¸°ë°˜ ìº¡ì…˜ ì„ë² ë”© ëª¨ë“ˆ
Googleì˜ EmbeddingGemma-300Mì„ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import config
from typing import List, Dict, Tuple


class CaptionEmbedder:
    """EmbeddingGemmaë¥¼ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, device: str = None):
        """
        CaptionEmbedder ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (EmbeddingGemma)
            device (str): ì‚¬ìš©í•  ì¥ì¹˜
        """
        if model_name is None:
            model_name = config.CAPTION_EMBEDDING_MODEL
        
        if device is None:
            device = config.get_device()
        
        self.device = device
        self.model_name = model_name
        
        # ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ ë° í† í° ì„¤ì •
        config.check_model_access(model_name)
        if not config.setup_huggingface_token():
            print("âš  Proceeding without token - some models may fail to load.")
        
        print(f"Loading EmbeddingGemma model: {model_name} on {device}...")
        try:
            # A100 GPU ìµœì í™” ì„¤ì •
            model_kwargs = {'device': str(device)}
            if device.type == 'cuda' and config.MIXED_PRECISION:
                print("  - Enabling mixed precision for caption embedding")
            
            self.model = SentenceTransformer(model_name, **model_kwargs)
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if device.type == 'cuda':
                print("  - GPU optimization enabled for caption embedder")
            
            print("EmbeddingGemma model loaded successfully.")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print("ğŸ’¡ Possible solutions:")
            print("  1. Set your Hugging Face token in config.py")
            print("  2. Accept the model license at: https://huggingface.co/google/embeddinggemma-300m")
            print("  3. Use alternative model: BAAI/bge-base-en-v1.5")
            raise
    
    def embed_caption(self, caption: str, task_type: str = "retrieval") -> np.ndarray:
        """
        ë‹¨ì¼ ìº¡ì…˜ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        EmbeddingGemmaì˜ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            caption (str): ìº¡ì…˜ í…ìŠ¤íŠ¸
            task_type (str): íƒœìŠ¤í¬ íƒ€ì… ("retrieval", "classification", "clustering", "similarity")
        
        Returns:
            numpy.ndarray: ì„ë² ë”© ë²¡í„°
        """
        # EmbeddingGemmaì˜ ë¬¸ì„œ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì ìš©
        if "embeddinggemma" in self.model_name.lower():
            # ë¬¸ì„œë¡œ ì²˜ë¦¬ (ê²€ìƒ‰ ëŒ€ìƒ)
            formatted_caption = f"title: none | text: {caption}"
            embedding = self.model.encode_document(formatted_caption, convert_to_numpy=True)
        else:
            # ì¼ë°˜ ì„ë² ë”© ëª¨ë¸
            embedding = self.model.encode(caption, convert_to_numpy=True)
        
        return embedding
    
    def embed_captions_batch(self, captions: List[str], batch_size: int = 32) -> np.ndarray:
        """
        ì—¬ëŸ¬ ìº¡ì…˜ì„ ë°°ì¹˜ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤.
        
        Args:
            captions (List[str]): ìº¡ì…˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
        
        Returns:
            numpy.ndarray: ì„ë² ë”© ë²¡í„°ë“¤ì˜ ë°°ì—´
        """
        print(f"Embedding {len(captions)} captions with EmbeddingGemma...")
        
        if "embeddinggemma" in self.model_name.lower():
            # EmbeddingGemmaì˜ ë¬¸ì„œ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì ìš©
            formatted_captions = [f"title: none | text: {caption}" for caption in captions]
            embeddings = self.model.encode_document(
                formatted_captions,
                convert_to_numpy=True,
                batch_size=batch_size,
                show_progress_bar=True
            )
        else:
            # ì¼ë°˜ ì„ë² ë”© ëª¨ë¸
            embeddings = self.model.encode(
                captions, 
                convert_to_numpy=True, 
                batch_size=batch_size,
                show_progress_bar=True
            )
        
        return embeddings
    
    def embed_query(self, query: str) -> np.ndarray:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.
        EmbeddingGemmaì˜ ì¿¼ë¦¬ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
        
        Returns:
            numpy.ndarray: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        """
        if "embeddinggemma" in self.model_name.lower():
            # EmbeddingGemmaì˜ ì¿¼ë¦¬ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì ìš©
            formatted_query = f"task: search result | query: {query}"
            embedding = self.model.encode_query(formatted_query, convert_to_numpy=True)
        else:
            # ì¼ë°˜ ì„ë² ë”© ëª¨ë¸
            embedding = self.model.encode(query, convert_to_numpy=True)
        
        return embedding


class CaptionSearcher:
    """ìº¡ì…˜ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self, caption_embedding_db):
        """
        CaptionSearcher ì´ˆê¸°í™”
        
        Args:
            caption_embedding_db: CaptionEmbeddingDB ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = caption_embedding_db
        self.db_indices, self.db_embeddings = self.db.get_all_embeddings()
        
        # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        if self.db_embeddings:
            self.db_embeddings = np.array(self.db_embeddings)
        else:
            self.db_embeddings = np.array([])
        
        print(f"CaptionSearcher initialized with {len(self.db_indices)} embeddings.")
    
    def search_by_text(self, query_embedding: np.ndarray, top_k: int = 10, 
                      threshold: float = None) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ìº¡ì…˜ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query_embedding (np.ndarray): ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
            top_k (int): ë°˜í™˜í•  ìƒìœ„ Kê°œ ê²°ê³¼
            threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if len(self.db_embeddings) == 0:
            return []
        
        if threshold is None:
            threshold = config.SIMILARITY_THRESHOLD
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.metrics.pairwise import cosine_similarity
        query_embedding = query_embedding.reshape(1, -1)
        similarities = cosine_similarity(query_embedding, self.db_embeddings)[0]
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í•„í„°ë§
        valid_indices = np.where(similarities >= threshold)[0]
        
        if len(valid_indices) == 0:
            return []
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_indices = valid_indices[np.argsort(similarities[valid_indices])[::-1]]
        
        # ìƒìœ„ Kê°œ ê²°ê³¼ ì¶”ì¶œ
        results = []
        for i in range(min(top_k, len(sorted_indices))):
            db_idx = sorted_indices[i]
            original_index = self.db_indices[db_idx]
            similarity_score = similarities[db_idx]
            
            # ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            metadata = self.db.metadata.get(original_index, {})
            
            results.append({
                'index': original_index,
                'similarity': float(similarity_score),
                'metadata': metadata
            })
        
        return results


def create_caption_embedder(model_name: str = None, device: str = None):
    """CaptionEmbedder ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    return CaptionEmbedder(model_name, device)


def create_caption_searcher(caption_embedding_db):
    """CaptionSearcher ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    return CaptionSearcher(caption_embedding_db)


def build_caption_embedding_db(caption_db, caption_embedding_db, batch_size: int = 32):
    """
    ìº¡ì…˜ DBë¡œë¶€í„° ìº¡ì…˜ ì„ë² ë”© DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    
    Args:
        caption_db: CaptionDB ì¸ìŠ¤í„´ìŠ¤
        caption_embedding_db: CaptionEmbeddingDB ì¸ìŠ¤í„´ìŠ¤
        batch_size (int): ë°°ì¹˜ í¬ê¸°
    
    Returns:
        CaptionEmbeddingDB: êµ¬ì¶•ëœ ìº¡ì…˜ ì„ë² ë”© DB
    """
    print("Building caption embedding database...")
    
    # ê¸°ì¡´ DB ë¡œë“œ ì‹œë„
    if caption_embedding_db.load_db():
        print("Using existing caption embedding database.")
        return caption_embedding_db
    
    # ì„ë² ë” ìƒì„±
    embedder = create_caption_embedder()
    
    # ëª¨ë“  ìº¡ì…˜ ê°€ì ¸ì˜¤ê¸°
    all_captions = caption_db.get_all_captions()
    
    if not all_captions:
        print("No captions found in caption database.")
        return caption_embedding_db
    
    # ìº¡ì…˜ë“¤ê³¼ ì¸ë±ìŠ¤ ì¶”ì¶œ
    indices = list(all_captions.keys())
    captions = list(all_captions.values())
    
    # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
    embeddings = embedder.embed_captions_batch(captions, batch_size)
    
    # DBì— ì¶”ê°€
    for i, index in enumerate(indices):
        embedding = embeddings[i].tolist()  # JSON ì €ì¥ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        metadata = caption_db.metadata.get(index, {})
        caption_embedding_db.add_embedding(index, embedding, metadata)
    
    # DB ì €ì¥
    caption_embedding_db.save_db()
    
    print("Caption embedding database built successfully.")
    return caption_embedding_db


def search_captions_by_text(query: str, caption_embedding_db, 
                           top_k: int = 10, threshold: float = None) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìº¡ì…˜ì„ ê²€ìƒ‰í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        query (str): ê²€ìƒ‰ ì¿¼ë¦¬
        caption_embedding_db: CaptionEmbeddingDB ì¸ìŠ¤í„´ìŠ¤
        top_k (int): ë°˜í™˜í•  ìƒìœ„ Kê°œ ê²°ê³¼
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
    
    Returns:
        List[Dict]: ê²€ìƒ‰ ê²°ê³¼
    """
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    embedder = create_caption_embedder()
    query_embedding = embedder.embed_query(query)
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    searcher = create_caption_searcher(caption_embedding_db)
    results = searcher.search_by_text(query_embedding, top_k, threshold)
    
    return results
