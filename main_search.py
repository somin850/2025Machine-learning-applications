#!/usr/bin/env python3
"""
Main Search Module - ì¿¼ë¦¬ ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ ë° Recall í‰ê°€
ê° ëª¨ë¸ë³„ ì„ë² ë”© DBì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ê³  Recall ì„±ëŠ¥ì„ í‰ê°€
"""

import json
import os
import numpy as np
from typing import Dict, List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime
from tqdm import tqdm
import argparse

# ê¸°ë³¸ ì„¤ì •
EMBEDDING_MODEL = "google/embeddinggemma-300m"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class ImageSearchEngine:
    """ì´ë¯¸ì§€ ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, embedding_db_path: str, model_name: str = EMBEDDING_MODEL, device: str = DEVICE):
        """
        ImageSearchEngine ì´ˆê¸°í™”
        
        Args:
            embedding_db_path (str): ì„ë² ë”© DB íŒŒì¼ ê²½ë¡œ
            model_name (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.embedding_db_path = embedding_db_path
        self.model_name = model_name
        self.device = device
        self.embedding_model = None
        
        # DB ë°ì´í„°
        self.embeddings = {}
        self.captions = {}
        self.embedding_matrix = None
        self.index_list = []
        
        print(f"ğŸ” Initializing Image Search Engine")
        print(f"   DB Path: {embedding_db_path}")
        print(f"   Model: {model_name}")
        print(f"   Device: {device}")
        
        self.load_embedding_db()
        
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self.embedding_model is None:
            print(f"ğŸ“¥ Loading embedding model: {self.model_name}")
            
            self.embedding_model = SentenceTransformer(
                self.model_name,
                device=self.device,
                trust_remote_code=True
            )
            
            # Mixed precision ì„¤ì • (A100 ìµœì í™”)
            if self.device == "cuda":
                self.embedding_model.half()
                print("   âš¡ Mixed precision enabled")
            
            print("   âœ… Embedding model loaded successfully")
    
    def load_embedding_db(self):
        """ì„ë² ë”© DB ë¡œë“œ"""
        print(f"ğŸ“‚ Loading embedding DB: {self.embedding_db_path}")
        
        if not os.path.exists(self.embedding_db_path):
            raise FileNotFoundError(f"Embedding DB not found: {self.embedding_db_path}")
        
        with open(self.embedding_db_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.embeddings = data.get('embeddings', {})
        self.captions = data.get('captions', {})
        
        if not self.embeddings:
            raise ValueError(f"No embeddings found in {self.embedding_db_path}")
        
        # ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
        self.index_list = []
        embedding_list = []
        
        for idx_str, embedding in self.embeddings.items():
            self.index_list.append(int(idx_str))
            embedding_list.append(np.array(embedding))
        
        self.embedding_matrix = np.vstack(embedding_list)
        
        print(f"   ğŸ“Š Loaded {len(self.embeddings)} embeddings")
        print(f"   ğŸ“ Embedding dimension: {self.embedding_matrix.shape[1]}")
        
    def embed_query(self, query: str) -> np.ndarray:
        """
        ì¿¼ë¦¬ë¥¼ ì„ë² ë”©
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            np.ndarray: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        """
        if self.embedding_model is None:
            self.load_embedding_model()
        
        # EmbeddingGemmaëŠ” query embeddingì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í¬ë§· ì‚¬ìš©
        formatted_query = f"Represent this query for retrieving relevant captions: {query}"
        
        with torch.no_grad():
            if self.device == "cuda":
                with torch.cuda.amp.autocast():
                    embedding = self.embedding_model.encode(
                        formatted_query,
                        convert_to_numpy=True,
                        normalize_embeddings=True
                    )
            else:
                embedding = self.embedding_model.encode(
                    formatted_query,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
        
        return embedding
    
    def search_similar_images(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embed_query(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‚´ì ìœ¼ë¡œ ê³„ì‚°)
        similarities = np.dot(self.embedding_matrix, query_embedding)
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for rank, idx in enumerate(top_indices):
            image_index = self.index_list[idx]
            similarity = float(similarities[idx])
            caption = self.captions.get(str(image_index), "No caption available")
            
            result = {
                "rank": rank + 1,
                "image_index": image_index,
                "similarity": similarity,
                "caption": caption,
                "image_path": f"flickr8k_train200/flickr_{image_index:05d}.jpg"  # ì´ë¯¸ì§€ ê²½ë¡œ í˜•ì‹
            }
            results.append(result)
        
        return results
    
    def batch_search(self, queries: Dict[str, str], top_k: int = 10) -> Dict[str, List[Dict]]:
        """
        ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ì¿¼ë¦¬ ê²€ìƒ‰
        
        Args:
            queries (Dict[str, str]): {query_id: query_text} í˜•íƒœì˜ ì¿¼ë¦¬ ë”•ì…”ë„ˆë¦¬
            top_k (int): ê° ì¿¼ë¦¬ë‹¹ ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            
        Returns:
            Dict[str, List[Dict]]: {query_id: search_results} í˜•íƒœì˜ ê²°ê³¼
        """
        results = {}
        
        print(f"ğŸ” Searching {len(queries)} queries with top-{top_k} results each")
        
        for query_id, query_text in tqdm(queries.items(), desc="Searching"):
            try:
                search_results = self.search_similar_images(query_text, top_k)
                results[query_id] = search_results
            except Exception as e:
                print(f"âŒ Error searching query {query_id}: {e}")
                results[query_id] = []
        
        return results

class RecallEvaluator:
    """Recall í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """RecallEvaluator ì´ˆê¸°í™”"""
        pass
    
    def calculate_recall(self, search_results: Dict[str, List[Dict]], 
                        ground_truth: Dict[str, int], 
                        k_values: List[int] = [1, 5, 10]) -> Dict:
        """
        Recall@K ê³„ì‚°
        
        Args:
            search_results (Dict): ê²€ìƒ‰ ê²°ê³¼ {query_id: [results]}
            ground_truth (Dict): ì •ë‹µ {query_id: correct_image_index}
            k_values (List[int]): í‰ê°€í•  K ê°’ë“¤
            
        Returns:
            Dict: Recall ê²°ê³¼
        """
        recall_results = {f"recall@{k}": [] for k in k_values}
        detailed_results = {}
        
        for query_id, results in search_results.items():
            if query_id not in ground_truth:
                continue
                
            correct_index = ground_truth[query_id]
            
            # ê° Kì— ëŒ€í•´ Recall ê³„ì‚°
            query_recalls = {}
            for k in k_values:
                top_k_indices = [result['image_index'] for result in results[:k]]
                is_correct = correct_index in top_k_indices
                recall_results[f"recall@{k}"].append(1.0 if is_correct else 0.0)
                query_recalls[f"recall@{k}"] = 1.0 if is_correct else 0.0
            
            detailed_results[query_id] = {
                "ground_truth": correct_index,
                "top_results": results[:max(k_values)],
                "recalls": query_recalls
            }
        
        # í‰ê·  Recall ê³„ì‚°
        avg_recalls = {}
        for k in k_values:
            recalls = recall_results[f"recall@{k}"]
            avg_recalls[f"recall@{k}"] = np.mean(recalls) if recalls else 0.0
        
        return {
            "average_recalls": avg_recalls,
            "detailed_results": detailed_results,
            "total_queries": len(search_results)
        }

def load_queries(query_file_path: str) -> Dict[str, str]:
    """
    ì¿¼ë¦¬ íŒŒì¼ ë¡œë“œ
    
    Args:
        query_file_path (str): ì¿¼ë¦¬ JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        Dict[str, str]: {query_id: query_text} í˜•íƒœì˜ ì¿¼ë¦¬ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ“‚ Loading queries from: {query_file_path}")
    
    with open(query_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ì¿¼ë¦¬ í˜•ì‹ì´ {"queries": {"0": "query text", ...}} ì¸ ê²½ìš°
    if 'queries' in data:
        queries = data['queries']
    # ì¿¼ë¦¬ í˜•ì‹ì´ {"0": "query text", ...} ì¸ ê²½ìš°
    else:
        queries = data
    
    print(f"   ğŸ“Š Loaded {len(queries)} queries")
    return queries

def create_ground_truth_from_queries(queries: Dict[str, str]) -> Dict[str, int]:
    """
    ì¿¼ë¦¬ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ground truth ìƒì„±
    (ì¿¼ë¦¬ IDê°€ ì •ë‹µ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ë¼ê³  ê°€ì •)
    
    Args:
        queries (Dict[str, str]): ì¿¼ë¦¬ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        Dict[str, int]: {query_id: correct_image_index} í˜•íƒœì˜ ground truth
    """
    ground_truth = {}
    for query_id in queries.keys():
        try:
            # ì¿¼ë¦¬ IDë¥¼ ì •ë‹µ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©
            ground_truth[query_id] = int(query_id)
        except ValueError:
            print(f"âš ï¸ Warning: Cannot convert query_id '{query_id}' to int")
            continue
    
    return ground_truth

def save_results(results: Dict, output_path: str):
    """
    ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        results (Dict): ì €ì¥í•  ê²°ê³¼
        output_path (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ Results saved to: {output_path}")

def run_search_evaluation(embedding_db_path: str, query_file_path: str, 
                         output_dir: str, model_type: str, top_k: int = 10):
    """
    ê²€ìƒ‰ ë° í‰ê°€ ì‹¤í–‰
    
    Args:
        embedding_db_path (str): ì„ë² ë”© DB íŒŒì¼ ê²½ë¡œ
        query_file_path (str): ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ
        output_dir (str): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_type (str): ëª¨ë¸ íƒ€ì… (ê²°ê³¼ íŒŒì¼ëª…ì— ì‚¬ìš©)
        top_k (int): ê²€ìƒ‰í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
    """
    print(f"\n{'='*20} {model_type} Search Evaluation {'='*20}")
    
    try:
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        search_engine = ImageSearchEngine(embedding_db_path)
        
        # ì¿¼ë¦¬ ë¡œë“œ
        queries = load_queries(query_file_path)
        
        # Ground truth ìƒì„± (ì¿¼ë¦¬ ID = ì •ë‹µ ì´ë¯¸ì§€ ì¸ë±ìŠ¤)
        ground_truth = create_ground_truth_from_queries(queries)
        
        # ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = search_engine.batch_search(queries, top_k)
        
        # Recall í‰ê°€
        evaluator = RecallEvaluator()
        recall_results = evaluator.calculate_recall(search_results, ground_truth)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š {model_type} Recall Results:")
        for metric, value in recall_results['average_recalls'].items():
            print(f"   {metric}: {value:.4f}")
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        final_results = {
            "model_info": {
                "model_type": model_type,
                "embedding_db_path": embedding_db_path,
                "query_file_path": query_file_path,
                "evaluation_time": datetime.now().isoformat(),
                "top_k": top_k
            },
            "search_results": search_results,
            "recall_evaluation": recall_results
        }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"{model_type}_search_results_{timestamp}.json")
        save_results(final_results, output_file)
        
        return final_results
        
    except Exception as e:
        print(f"âŒ Error in {model_type} evaluation: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Image Search and Recall Evaluation")
    parser.add_argument("--query_file", type=str, required=True, 
                       help="Path to query JSON file")
    parser.add_argument("--output_dir", type=str, default="search_results",
                       help="Output directory for results")
    parser.add_argument("--top_k", type=int, default=10,
                       help="Number of top results to retrieve")
    parser.add_argument("--models", nargs='+', 
                       choices=['blip_base', 'blip_large', 'vit_gpt2', 'vlm', 'vlm_wosimilar', 'all'],
                       default=['all'],
                       help="Models to evaluate")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ë³„ ì„ë² ë”© DB ê²½ë¡œ
    embedding_dbs = {
        'blip_base': "personalized_DB_Embedding/blip_base_embeddings.json",
        'blip_large': "personalized_DB_Embedding/blip_large_embeddings.json",
        'vit_gpt2': "personalized_DB_Embedding/vit_gpt2_embeddings.json",
        'vlm': "personalized_DB_Embedding/VLM_embeddings.json",
        'vlm_wosimilar': "personalized_DB_Embedding/VLM_wosimilar_embeddings.json"
    }
    
    # í‰ê°€í•  ëª¨ë¸ ê²°ì •
    if 'all' in args.models:
        models_to_evaluate = list(embedding_dbs.keys())
    else:
        models_to_evaluate = args.models
    
    print("=" * 80)
    print("ğŸ” Image Search and Recall Evaluation")
    print("=" * 80)
    print(f"ğŸ“‚ Query file: {args.query_file}")
    print(f"ğŸ“ Output directory: {args.output_dir}")
    print(f"ğŸ”¢ Top-K: {args.top_k}")
    print(f"ğŸ¤– Models: {', '.join(models_to_evaluate)}")
    print(f"ğŸ’» Device: {DEVICE}")
    print()
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA Available: {torch.cuda.get_device_name()}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    all_results = {}
    
    # ê° ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰
    for model_type in models_to_evaluate:
        if model_type not in embedding_dbs:
            print(f"âŒ Unknown model type: {model_type}")
            continue
            
        embedding_db_path = embedding_dbs[model_type]
        
        if not os.path.exists(embedding_db_path):
            print(f"âŒ Embedding DB not found: {embedding_db_path}")
            continue
        
        results = run_search_evaluation(
            embedding_db_path=embedding_db_path,
            query_file_path=args.query_file,
            output_dir=args.output_dir,
            model_type=model_type,
            top_k=args.top_k
        )
        
        if results:
            all_results[model_type] = results['recall_evaluation']['average_recalls']
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if all_results:
        print("\n" + "=" * 80)
        print("ğŸ“Š Final Recall Comparison")
        print("=" * 80)
        
        # í—¤ë” ì¶œë ¥
        print(f"{'Model':<15} {'Recall@1':<12} {'Recall@5':<12} {'Recall@10':<12}")
        print("-" * 55)
        
        # ê° ëª¨ë¸ ê²°ê³¼ ì¶œë ¥
        for model_type, recalls in all_results.items():
            r1 = recalls.get('recall@1', 0.0)
            r5 = recalls.get('recall@5', 0.0)
            r10 = recalls.get('recall@10', 0.0)
            print(f"{model_type:<15} {r1:<12.4f} {r5:<12.4f} {r10:<12.4f}")
        
        # ì¢…í•© ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_file = os.path.join(args.output_dir, f"recall_summary_{timestamp}.json")
        
        summary_data = {
            "evaluation_info": {
                "query_file": args.query_file,
                "top_k": args.top_k,
                "evaluation_time": datetime.now().isoformat(),
                "models_evaluated": list(all_results.keys())
            },
            "recall_results": all_results
        }
        
        save_results(summary_data, summary_file)
        
        print(f"\nğŸ‰ Evaluation completed! Summary saved to: {summary_file}")
    else:
        print("\nâŒ No successful evaluations!")

if __name__ == "__main__":
    main()
