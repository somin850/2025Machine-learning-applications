# ğŸ“„ main.py
"""
Image Search í”„ë¡œì íŠ¸ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import os
import json
from datetime import datetime
import config

# OpenMP ì˜¤ë¥˜ í•´ê²°
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
from dataset_loader import load_and_split_dataset
from image_embedder import build_image_embedding_db
from similarity_search import search_similar_images
from vlm_captioner import generate_caption_with_similarity
from db_manager import create_database_manager, initialize_databases_from_training_data
from caption_embedder import build_caption_embedding_db


def setup_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    directories = [
        config.DATA_DIR,
        config.RESULTS_DIR
    ]
    
    for directory in directories:
        if directory:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            os.makedirs(directory, exist_ok=True)
            print(f"âœ“ Created directory: {directory}")


def initialize_system():
    """ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("=" * 60)
    print("ğŸš€ Image Search System Initialization")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    setup_directories()
    
    # Hugging Face í† í° ì„¤ì •
    print("\n--- Step 0: Hugging Face Authentication ---")
    config.setup_huggingface_token()
    
    # ëœë¤ ì‹œë“œ ì„¤ì •
    config.set_random_seed()
    
    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„ë¦¬
    print("\n--- Step 1: Loading and Splitting Dataset ---")
    dataset_loader = load_and_split_dataset()
    
    # í›ˆë ¨ ë°ì´í„°ì™€ ì‹¤í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    training_data = dataset_loader.get_training_data()
    experiment_data = dataset_loader.get_experiment_data()
    
    print(f"âœ“ Training data: {len(training_data)} samples")
    print(f"âœ“ Experiment data: {len(experiment_data)} samples")
    
    # ì´ë¯¸ì§€ ì„ë² ë”© DB êµ¬ì¶•
    print("\n--- Step 2: Building Image Embedding Database ---")
    image_embedding_db = build_image_embedding_db(training_data)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    print("\n--- Step 3: Initializing Database Manager ---")
    db_manager = create_database_manager()
    db_manager.set_image_embedding_db(image_embedding_db)
    
    # ìº¡ì…˜ DB ì´ˆê¸°í™”
    initialize_databases_from_training_data(training_data, db_manager)
    
    # ìº¡ì…˜ ì„ë² ë”© DBëŠ” ì²˜ìŒì—ëŠ” ë¹„ì–´ìˆìŒ (ìƒì„±ëœ ìº¡ì…˜ë§Œ ì €ì¥)
    print("\n--- Step 4: Initializing Caption Embedding Database ---")
    print("  Caption Embedding DB initialized as empty (will store generated captions only)")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” í™•ì¸
    print("\n--- Step 5: Database Synchronization Check ---")
    db_manager.sync_databases()
    
    print("\nâœ… System initialization completed successfully!")
    print("=" * 60)
    
    return dataset_loader, db_manager


def run_experiment(dataset_loader, db_manager, experiment_index: int = None):
    """ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if experiment_index is None:
        experiment_index = config.EXPERIMENT_IMAGE_INDEX
    
    print(f"\nğŸ” Running Experiment with Image Index: {experiment_index}")
    print("=" * 60)
    
    # ì‹¤í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    experiment_data = dataset_loader.get_experiment_data()
    
    if experiment_index >= len(experiment_data):
        raise ValueError(f"Experiment index {experiment_index} out of range. Max: {len(experiment_data) - 1}")
    
    # Step 1: ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²€ìƒ‰
    print("\n--- Step 1: Finding Similar Images ---")
    search_result = search_similar_images(
        experiment_data=experiment_data,
        experiment_index=experiment_index,
        image_embedding_db=db_manager.image_embedding_db,
        top_k=config.TOP_K_SIMILAR
    )
    
    # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    query_info = search_result['query_info']
    similar_images = search_result['similar_images']
    
    print(f"Query Image Info:")
    print(f"  - Experiment Index: {query_info['experiment_index']}")
    print(f"  - Original Index: {query_info['original_index']}")
    print(f"  - Original Caption: {query_info['caption']}")
    
    print(f"\nTop {len(similar_images)} Similar Images:")
    for i, img_info in enumerate(similar_images, 1):
        print(f"  {i}. Index: {img_info['index']}, Similarity: {img_info['similarity']:.4f}")
        print(f"     Caption: {img_info['metadata']['caption']}")
    
    # Step 2: VLMìœ¼ë¡œ ìº¡ì…˜ ìƒì„±
    print("\n--- Step 2: Generating Caption with VLM ---")
    query_image = experiment_data[experiment_index]['image']
    
    generated_caption = generate_caption_with_similarity(
        image=query_image,
        search_result=search_result,
        max_new_tokens=config.MAX_LENGTH,
        temperature=config.TEMPERATURE,
        db_manager=db_manager
    )
    
    print(f"Generated Caption: {generated_caption}")
    
    # Step 3: ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DBì— ì¶”ê°€
    print("\n--- Step 3: Adding New Data to Databases ---")
    
    # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„± (ê¸°ì¡´ ìµœëŒ€ ì¸ë±ìŠ¤ + 1)
    existing_indices = list(db_manager.caption_db.get_all_captions().keys())
    new_index = max(existing_indices) + 1 if existing_indices else 0
    
    # ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
    from image_embedder import create_image_embedder
    image_embedder = create_image_embedder()
    new_image_embedding = image_embedder.embed_image(query_image)
    
    # ìº¡ì…˜ ì„ë² ë”© ìƒì„± (ìƒì„±ëœ ìº¡ì…˜ë§Œ)
    from caption_embedder import create_caption_embedder
    caption_embedder = create_caption_embedder()
    new_caption_embedding = caption_embedder.embed_caption(generated_caption)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        'original_experiment_index': experiment_index,
        'original_index': query_info['original_index'],
        'original_caption': query_info['caption'],
        'generation_timestamp': datetime.now().isoformat(),
        'similar_images_used': [img['index'] for img in similar_images]
    }
    
    # ëª¨ë“  DBì— ì¶”ê°€
    db_manager.add_new_data(
        index=new_index,
        image_embedding=new_image_embedding.tolist(),
        caption=generated_caption,
        caption_embedding=new_caption_embedding.tolist(),
        metadata=metadata
    )
    
    print(f"âœ“ New data added with index: {new_index}")
    
    # Step 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    print("\n--- Step 4: Saving Updated Databases ---")
    db_manager.save_all_databases()
    
    # Step 5: ê²°ê³¼ ì €ì¥
    print("\n--- Step 5: Saving Experiment Results ---")
    experiment_result = {
        'experiment_info': {
            'experiment_index': experiment_index,
            'timestamp': datetime.now().isoformat(),
            'new_db_index': new_index
        },
        'query_info': query_info,
        'similar_images': similar_images,
        'generated_caption': generated_caption,
        'original_caption': query_info['caption'],
        'metadata': metadata
    }
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    result_file = os.path.join(config.RESULTS_DIR, f"experiment_{experiment_index}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_result, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ Experiment results saved: {result_file}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¶œë ¥
    print("\n--- Final Database Statistics ---")
    stats = db_manager.get_database_stats()
    for db_name, count in stats.items():
        print(f"  - {db_name}: {count} entries")
    
    print("\nâœ… Experiment completed successfully!")
    print("=" * 60)
    
    return experiment_result


def run_all_experiments(dataset_loader, db_manager):
    """0~399ê¹Œì§€ ëª¨ë“  ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("\nğŸ”„ Starting Sequential Experiments (0~399)")
    print("=" * 60)
    
    experiment_data = dataset_loader.get_experiment_data()
    total_experiments = len(experiment_data)
    
    print(f"Total experiments to run: {total_experiments}")
    
    results_summary = []
    
    for i in range(total_experiments):
        print(f"\nğŸ“Š Running Experiment {i+1}/{total_experiments} (Index: {i})")
        print("-" * 50)
        
        try:
            # ê° ì‹¤í—˜ ì‹¤í–‰
            experiment_result = run_experiment(dataset_loader, db_manager, experiment_index=i)
            
            # ê²°ê³¼ ìš”ì•½ ì €ì¥
            summary = {
                'experiment_index': i,
                'success': True,
                'generated_caption': experiment_result['generated_caption'],
                'original_caption': experiment_result['original_caption'],
                'timestamp': experiment_result['experiment_info']['timestamp']
            }
            results_summary.append(summary)
            
            print(f"âœ… Experiment {i} completed successfully")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA ì‚¬ìš© ì‹œ)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"âŒ Experiment {i} failed: {e}")
            summary = {
                'experiment_index': i,
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            results_summary.append(summary)
            continue
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½ ì €ì¥
    summary_file = os.path.join(config.RESULTS_DIR, f"experiments_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    # í†µê³„ ì¶œë ¥
    successful = sum(1 for r in results_summary if r['success'])
    failed = total_experiments - successful
    
    print(f"\nğŸ“ˆ Experiments Summary:")
    print(f"  - Total: {total_experiments}")
    print(f"  - Successful: {successful}")
    print(f"  - Failed: {failed}")
    print(f"  - Success Rate: {successful/total_experiments*100:.1f}%")
    print(f"  - Summary saved: {summary_file}")
    
    return results_summary

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # GPU ë©”ëª¨ë¦¬ ì„¤ì •
        config.setup_gpu_memory()
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        dataset_loader, db_manager = initialize_system()
        
        # ëª¨ë“  ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰
        results_summary = run_all_experiments(dataset_loader, db_manager)
        
        print(f"\nğŸ‰ All experiments completed!")
        
        # ìµœì¢… ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        print("\nğŸ“Š Final Database Statistics:")
        stats = db_manager.get_database_stats()
        for db_name, count in stats.items():
            print(f"  - {db_name}: {count} entries")
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
