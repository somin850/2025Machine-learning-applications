# ğŸ“„ blip_base_captioner.py
"""
BLIP Base ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 400ê°œ ì‹¤í—˜ ì´ë¯¸ì§€ì˜ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” blip_base_captions.json íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.
"""

import os
import json
from datetime import datetime
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import config
from dataset_loader import Flickr8KLoader

class BlipBaseCaptioner:
    """BLIP Base ëª¨ë¸ì„ ì‚¬ìš©í•œ ìº¡ì…˜ ìƒì„±ê¸°"""
    
    def __init__(self, device=None):
        """
        BlipBaseCaptioner ì´ˆê¸°í™”
        
        Args:
            device: ì‚¬ìš©í•  ì¥ì¹˜ (cuda ë˜ëŠ” cpu)
        """
        if device is None:
            device = config.get_device()
        
        self.device = device
        self.model_name = "Salesforce/blip-image-captioning-base"
        
        print(f"Loading BLIP Base model: {self.model_name} on {device}...")
        
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = BlipProcessor.from_pretrained(self.model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)
        self.model.to(device)
        self.model.eval()
        
        print("BLIP Base model loaded successfully.")
    
    def generate_caption(self, image, max_length=50):
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            max_length: ìµœëŒ€ ìº¡ì…˜ ê¸¸ì´
            
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("image must be PIL Image or image path")
        
        # ì…ë ¥ ì „ì²˜ë¦¬
        inputs = self.processor(image, return_tensors="pt").to(self.device)
        
        # ìº¡ì…˜ ìƒì„±
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_length=max_length,
                num_beams=4,
                early_stopping=True,
                do_sample=False
            )
        
        # í…ìŠ¤íŠ¸ ë””ì½”ë”©
        caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
        
        return caption
    
    def generate_captions_for_experiments(self, experiment_data, output_file="blip_base_captions.json"):
        """
        ì‹¤í—˜ ë°ì´í„°ì˜ ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            experiment_data: ì‹¤í—˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            dict: ìƒì„±ëœ ìº¡ì…˜ ë°ì´í„°
        """
        print(f"\nğŸ¯ Generating captions for {len(experiment_data)} experiment images...")
        print(f"Model: {self.model_name}")
        print(f"Output file: {output_file}")
        print("=" * 60)
        
        captions_data = {
            "captions": {},
            "metadata": {},
            "total_captions": 0,
            "model_info": {
                "model_name": self.model_name,
                "generation_timestamp": datetime.now().isoformat(),
                "total_experiments": len(experiment_data)
            }
        }
        
        for i, item in enumerate(experiment_data):
            try:
                print(f"\nğŸ“¸ Processing experiment {i}/{len(experiment_data)-1} (Index: {i})")
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = item['image']
                original_caption = item['caption']
                original_index = item['original_index']
                
                # ìº¡ì…˜ ìƒì„±
                generated_caption = self.generate_caption(image)
                
                print(f"  Original: {original_caption}")
                print(f"  Generated: {generated_caption}")
                
                # ë°ì´í„° ì €ì¥
                experiment_index = str(i)  # ì‹¤í—˜ ì¸ë±ìŠ¤ë¥¼ í‚¤ë¡œ ì‚¬ìš©
                
                captions_data["captions"][experiment_index] = generated_caption
                captions_data["metadata"][experiment_index] = {
                    "experiment_index": i,
                    "original_index": original_index,
                    "original_caption": original_caption,
                    "generation_timestamp": datetime.now().isoformat(),
                    "model_name": self.model_name
                }
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âŒ Error processing experiment {i}: {e}")
                continue
        
        # ì´ ìº¡ì…˜ ìˆ˜ ì—…ë°ì´íŠ¸
        captions_data["total_captions"] = len(captions_data["captions"])
        
        # íŒŒì¼ ì €ì¥
        print(f"\nğŸ’¾ Saving results to {output_file}...")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(captions_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Successfully generated {captions_data['total_captions']} captions")
        print(f"ğŸ“ Results saved to: {output_file}")
        
        return captions_data


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # GPU ë©”ëª¨ë¦¬ ì„¤ì •
        config.setup_gpu_memory()
        
        print("ğŸš€ BLIP Base Caption Generation")
        print("=" * 60)
        
        # ë°ì´í„°ì…‹ ë¡œë” ì´ˆê¸°í™”
        print("\n--- Step 1: Loading Dataset ---")
        dataset_loader = Flickr8KLoader()
        dataset_loader.load_dataset()
        dataset_loader.load_split_data()
        
        # ì‹¤í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        experiment_data = dataset_loader.get_experiment_data()
        print(f"âœ“ Loaded {len(experiment_data)} experiment images")
        
        # BLIP Base ìº¡ì…”ë„ˆ ì´ˆê¸°í™”
        print("\n--- Step 2: Initializing BLIP Base Model ---")
        captioner = BlipBaseCaptioner()
        
        # ìº¡ì…˜ ìƒì„±
        print("\n--- Step 3: Generating Captions ---")
        results = captioner.generate_captions_for_experiments(
            experiment_data, 
            output_file="blip_base_captions.json"
        )
        
        print(f"\nğŸ‰ BLIP Base caption generation completed!")
        print(f"ğŸ“Š Statistics:")
        print(f"  - Total experiments: {len(experiment_data)}")
        print(f"  - Successful captions: {results['total_captions']}")
        print(f"  - Success rate: {results['total_captions']/len(experiment_data)*100:.1f}%")
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
