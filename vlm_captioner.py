# ğŸ“„ vlm_captioner.py
"""
SmolVLM ê¸°ë°˜ ìº¡ì…˜ ìƒì„± ëª¨ë“ˆ
í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ SmolVLMìœ¼ë¡œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
SmolVLMì€ 2B íŒŒë¼ë¯¸í„°ì˜ íš¨ìœ¨ì ì¸ Vision Language Modelì…ë‹ˆë‹¤.
"""

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import config


class VLMCaptioner:
    """SmolVLMì„ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, device: str = None):
        """
        VLMCaptioner ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  SmolVLM ëª¨ë¸ ì´ë¦„
            device (str): ì‚¬ìš©í•  ì¥ì¹˜
        """
        if model_name is None:
            model_name = config.VLM_MODEL_NAME
        
        if device is None:
            device = config.get_device()
        
        self.device = device
        self.model_name = model_name
        
        # ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ ë° í† í° ì„¤ì •
        config.check_model_access(model_name)
        if not config.setup_huggingface_token():
            print("âš  Proceeding without token - some models may fail to load.")
        
        print(f"Loading SmolVLM model: {model_name} on {device}...")
        
        try:
            # SmolVLM ëª¨ë¸ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(model_name)
            
            # A100 GPU ìµœì í™” ì„¤ì •
            model_kwargs = {
                "torch_dtype": torch.bfloat16 if device.type == "cuda" else torch.float32,
            }
            
            # ê¸°ë³¸ attention ì‚¬ìš© (ì•ˆì •ì„± ìš°ì„ )
            model_kwargs["_attn_implementation"] = "eager"
            
            if device.type == "cuda":
                print("  - Using eager attention for stable performance")
                
                # A100 MIGì—ì„œ ë©”ëª¨ë¦¬ ìµœì í™”
                model_kwargs["low_cpu_mem_usage"] = True
                model_kwargs["device_map"] = "auto"
                print("  - Memory optimization enabled for A100 MIG")
            
            self.model = AutoModelForVision2Seq.from_pretrained(model_name, **model_kwargs)
            
            if device.type != "cuda":  # device_map="auto"ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìˆ˜ë™ ì´ë™
                self.model = self.model.to(device)
            
            self.model.eval()
            print("SmolVLM model loaded successfully.")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print("ğŸ’¡ Possible solutions:")
            print("  1. Set your Hugging Face token in config.py")
            print("  2. Accept the model license at: https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct")
            print("  3. Use alternative model: Salesforce/blip-image-captioning-large")
            raise
    
    def generate_caption(self, image, prompt: str = None, 
                        max_new_tokens: int = None, temperature: float = None) -> str:
        """
        ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            prompt (str): í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
        
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        if max_new_tokens is None:
            max_new_tokens = config.MAX_LENGTH
        if temperature is None:
            temperature = config.TEMPERATURE
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            image = Image.open(image)
        elif not isinstance(image, Image.Image):
            raise ValueError("image must be PIL Image or image path")
        
        # SmolVLMìš© ë©”ì‹œì§€ í˜•ì‹ êµ¬ì„±
        if prompt:
            # í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ” ê²½ìš°
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        else:
            # ê¸°ë³¸ ìº¡ì…˜ ìƒì„± í”„ë¡¬í”„íŠ¸
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": "Describe this image in detail. Provide a clear, concise caption in English that describes the main objects, people, actions, and setting visible in the image."}
                    ]
                }
            ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)
        
        # ì…ë ¥ ì „ì²˜ë¦¬
        inputs = self.processor(text=formatted_prompt, images=[image], return_tensors="pt")
        inputs = inputs.to(self.device)
        
        # ìº¡ì…˜ ìƒì„±
        with torch.no_grad():
            if temperature > 0:
                # ì˜¨ë„ ê¸°ë°˜ ìƒ˜í”Œë§
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True
                )
            else:
                # ê·¸ë¦¬ë”” ë””ì½”ë”©
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
        
        # ë””ì½”ë”©
        generated_texts = self.processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )
        
        # ì‘ë‹µì—ì„œ ìº¡ì…˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        full_response = generated_texts[0]
        # SmolVLMì˜ ì‘ë‹µì—ì„œ ì‹¤ì œ ìº¡ì…˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "Assistant:" in full_response:
            caption = full_response.split("Assistant:")[-1].strip()
        else:
            caption = full_response.strip()
        
        return caption
    
    def generate_caption_with_context(self, image, similar_captions: list, 
                                    max_new_tokens: int = None, temperature: float = None) -> str:
        """
        ìœ ì‚¬í•œ ìº¡ì…˜ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            similar_captions (list): ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ì˜ ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸
            max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
        
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        from prompt_generator import create_prompt_generator
        prompt_generator = create_prompt_generator()
        prompt = prompt_generator.generate_prompt(similar_captions, debug=True)
        
        # ìº¡ì…˜ ìƒì„±
        caption = self.generate_caption(
            image=image,
            prompt=prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature
        )
        
        return caption
    
    def generate_caption_from_search_result(self, image, search_result: dict, 
                                          max_new_tokens: int = None, temperature: float = None,
                                          db_manager=None, similarity_threshold: float = 0.75) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  í”„ë¡¬í”„íŠ¸ë¡œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì˜ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ Top 3 ìº¡ì…˜ì„ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            search_result (dict): similarity_searchì—ì„œ ë°˜í™˜ëœ ê²€ìƒ‰ ê²°ê³¼
            max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
            db_manager: DatabaseManager ì¸ìŠ¤í„´ìŠ¤ (ìº¡ì…˜ ê²€ìƒ‰ìš©)
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.75)
        
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        # ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
        from prompt_generator import generate_vlm_prompt
        
        prompt = generate_vlm_prompt(
            search_result=search_result,
            db_manager=db_manager,
            similarity_threshold=similarity_threshold,
            max_captions=3  # Top 3 ìº¡ì…˜ë§Œ ì‚¬ìš©
        )
        
        print(f"\nğŸ“ Generated Prompt Strategy:")
        similar_images = search_result.get('similar_images', [])
        high_similarity_count = sum(1 for img in similar_images if img.get('similarity', 0.0) >= similarity_threshold)
        
        if high_similarity_count > 0:
            print(f"  - Using {min(high_similarity_count, 3)} high-similarity captions (threshold: {similarity_threshold})")
            print(f"  - Prompt includes similar image examples")
        else:
            print(f"  - No images above threshold ({similarity_threshold})")
            print(f"  - Using default prompt without examples")
        
        # ìº¡ì…˜ ìƒì„±
        return self.generate_caption(
            image=image,
            prompt=prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature
        )


class AdvancedVLMCaptioner(VLMCaptioner):
    """ê³ ê¸‰ SmolVLM ìº¡ì…˜ ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, device: str = None):
        """AdvancedVLMCaptioner ì´ˆê¸°í™”"""
        super().__init__(model_name, device)
    
    def generate_multiple_captions(self, image, prompt: str = None, 
                                 num_candidates: int = 3,
                                 max_new_tokens: int = None, temperature: float = 0.7) -> list:
        """
        ì—¬ëŸ¬ ê°œì˜ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            prompt (str): í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            num_candidates (int): ìƒì„±í•  ìº¡ì…˜ ê°œìˆ˜
            max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
        
        Returns:
            list: ìƒì„±ëœ ìº¡ì…˜ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        captions = []
        
        # ì—¬ëŸ¬ ë²ˆ ìƒì„±í•˜ì—¬ ë‹¤ì–‘í•œ ìº¡ì…˜ íšë“
        for i in range(num_candidates):
            # ê° ìƒì„±ë§ˆë‹¤ ì•½ê°„ ë‹¤ë¥¸ ì˜¨ë„ ì‚¬ìš©
            current_temp = temperature + (i * 0.1)
            caption = self.generate_caption(
                image=image,
                prompt=prompt,
                max_new_tokens=max_new_tokens,
                temperature=current_temp
            )
            captions.append(caption)
        
        return captions
    
    def generate_best_caption(self, image, similar_captions: list,
                            num_candidates: int = 3,
                            max_new_tokens: int = None, temperature: float = None) -> dict:
        """
        ì—¬ëŸ¬ í›„ë³´ ì¤‘ì—ì„œ ìµœê³ ì˜ ìº¡ì…˜ì„ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            similar_captions (list): ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ì˜ ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸
            num_candidates (int): í›„ë³´ ìº¡ì…˜ ê°œìˆ˜
            max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
        
        Returns:
            dict: {'best_caption': str, 'all_candidates': list}
        """
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        from prompt_generator import create_prompt_generator
        prompt_generator = create_prompt_generator()
        prompt = prompt_generator.generate_prompt(similar_captions)
        
        # ì—¬ëŸ¬ ìº¡ì…˜ ìƒì„±
        candidates = self.generate_multiple_captions(
            image=image,
            prompt=prompt,
            num_candidates=num_candidates,
            max_new_tokens=max_new_tokens,
            temperature=temperature if temperature else 0.7
        )
        
        # ì²« ë²ˆì§¸ ìº¡ì…˜ì„ ìµœê³ ë¡œ ì„ íƒ (ê°€ì¥ ì¼ê´€ëœ ê²°ê³¼)
        best_caption = candidates[0] if candidates else ""
        
        return {
            'best_caption': best_caption,
            'all_candidates': candidates
        }


def create_vlm_captioner(model_name: str = None, device: str = None):
    """VLMCaptioner ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    return VLMCaptioner(model_name, device)


def create_advanced_vlm_captioner(model_name: str = None, device: str = None):
    """AdvancedVLMCaptioner ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    return AdvancedVLMCaptioner(model_name, device)


def generate_caption_with_similarity(image, search_result: dict, 
                                   model_name: str = None, device: str = None,
                                   max_new_tokens: int = None, temperature: float = None,
                                   db_manager=None, similarity_threshold: float = 0.75) -> str:
    """
    ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  í”„ë¡¬í”„íŠ¸ë¡œ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
        search_result (dict): ê²€ìƒ‰ ê²°ê³¼
        model_name (str): SmolVLM ëª¨ë¸ ì´ë¦„
        device (str): ì‚¬ìš©í•  ì¥ì¹˜
        max_new_tokens (int): ìµœëŒ€ ìƒˆ í† í° ìˆ˜
        temperature (float): ìƒì„± ì˜¨ë„
        db_manager: DatabaseManager ì¸ìŠ¤í„´ìŠ¤ (ìº¡ì…˜ ê²€ìƒ‰ìš©)
        similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.75)
    
    Returns:
        str: ìƒì„±ëœ ìº¡ì…˜
    """
    captioner = create_vlm_captioner(model_name, device)
    return captioner.generate_caption_from_search_result(
        image=image,
        search_result=search_result,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        db_manager=db_manager,
        similarity_threshold=similarity_threshold
    )
